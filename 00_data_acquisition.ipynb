# 00_data_acquisition.ipynb
%pip install -q kaggle datasets gitpython pandas pyarrow tqdm

import os, json, shutil, zipfile, glob, re
from pathlib import Path
import pandas as pd
from tqdm.auto import tqdm

BASE = Path.cwd()
RAW  = BASE / "data" / "raw"
RAW.mkdir(parents=True, exist_ok=True)

# ---------- 0) (One-time) configure Kaggle API ----------
KAGGLE_TOKEN_JSON = None  # <-- paste your kaggle.json content as a Python dict or leave None if already configured
# Example:
# KAGGLE_TOKEN_JSON = {
#   "username": "your_kaggle_username",
#   "key": "xxxxxxxxxxxxxxxxxxxx"
# }
if KAGGLE_TOKEN_JSON:
    kagdir = Path.home()/".kaggle"
    kagdir.mkdir(parents=True, exist_ok=True)
    with open(kagdir/"kaggle.json", "w") as f:
        json.dump(KAGGLE_TOKEN_JSON, f)
    os.chmod(kagdir/"kaggle.json", 0o600)

# ---------- 1) LendingClub (Kaggle) ----------
# Source: https://www.kaggle.com/datasets/wordsforthewise/lending-club
# This dataset is big & split into multiple files; we’ll demonstrate on key CSVs if present.
# Tip: For a granting-only curated version and binary target mapping, see Zenodo reference. (Manual guidance)
!kaggle datasets download -d wordsforthewise/lending-club -p {RAW} -q

# Unzip all zips dropped by Kaggle
for z in RAW.glob("*.zip"):
    with zipfile.ZipFile(z, "r") as zip_ref:
        zip_ref.extractall(RAW)

# Try to find a consolidated loans CSV; fall back to concatenating year files if needed.
cand = list(RAW.glob("*.csv")) + list(RAW.glob("**/*.csv"))
print("LendingClub CSV candidates found:", len(cand))

# Save a path for a "main" file if you have curated one
LENDING_OUT = RAW / "lendingclub.csv"
if not LENDING_OUT.exists():
    # naive merge of any 'accepted' loan files containing loan_status / desc-like columns
    frames = []
    for c in cand:
        head = pd.read_csv(c, nrows=20, low_memory=False)
        if set(head.columns).intersection({"loan_status","desc","title","purpose"}):
            try:
                df_part = pd.read_csv(c, low_memory=False)
                frames.append(df_part)
            except Exception as e:
                print("Skip", c.name, e)
    if frames:
        df = pd.concat(frames, ignore_index=True)
        # normalize 'description' column best-effort
        if "desc" in df.columns:
            df["description"] = df["desc"]
        elif "title" in df.columns:
            df["description"] = df["title"].astype(str)
        else:
            df["description"] = ""
        df.to_csv(LENDING_OUT, index=False)
        print("Saved:", LENDING_OUT)

# ---------- 2) German Credit (Kaggle copy of Statlog) ----------
# Source: https://www.kaggle.com/datasets/prena0808/statlog-german-credit-data
!kaggle datasets download -d prena0808/statlog-german-credit-data -p {RAW} -q
for z in RAW.glob("statlog-german-credit-data*.zip"):
    with zipfile.ZipFile(z, "r") as zip_ref:
        zip_ref.extractall(RAW)
print("German Credit files:", list(RAW.glob("*german*.*"))[:5])

# ---------- 3) MultiFin (Hugging Face) ----------
# Source: https://huggingface.co/datasets/awinml/MultiFin
from datasets import load_dataset
mf = load_dataset("awinml/MultiFin")  # default = 'all_languages_highlevel'
# Flatten to CSV of text-only corpus for embeddings
mf_df = pd.DataFrame(mf["train"])
mf_all = pd.concat([pd.DataFrame(mf[s]) for s in mf], ignore_index=True)  # merge splits
mf_all.rename(columns={"text":"text"}, inplace=True)
mf_all.to_csv(RAW/"multifin_text.csv", index=False)
print("Saved:", RAW/"multifin_text.csv", "rows:", len(mf_all))

# ---------- 4) MultiFinBen (HF) – EnglishOCR & SpanishOCR ----------
# Sources: https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR
#          https://huggingface.co/collections/TheFinAI/multifinben-6826f6fc4bc13d8af4fab223
mben_en = load_dataset("TheFinAI/MultiFinBen-EnglishOCR", split="train")  # has 'text' field
mben_es = load_dataset("TheFinAI/MultiFinBen-SpanishOCR", split="train")  # same structure (collection page lists ES)
pd.DataFrame(mben_en)["text"].to_csv(RAW/"multifinben_en_text.csv", index=False)
pd.DataFrame(mben_es)["text"].to_csv(RAW/"multifinben_es_text.csv", index=False)

# ---------- 5) MAEC (GitHub) – transcripts only ----------
# Source: https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction
%pip install -q git+https://github.com/gitpython-developers/GitPython
from git import Repo
MAEC_DIR = RAW / "MAEC"
if not MAEC_DIR.exists():
    Repo.clone_from("https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction", MAEC_DIR)

# Extract transcripts from MAEC_Dataset/**/text.txt into a single CSV
rows = []
for textf in MAEC_DIR.glob("MAEC_Dataset/*/text.txt"):
    try:
        text = Path(textf).read_text(encoding="utf-8", errors="ignore")
        rows.append({"text": text, "source": textf.parent.name})
    except Exception as e:
        print("read error:", textf, e)

maec_df = pd.DataFrame(rows)
maec_df.to_csv(RAW/"maec_transcripts.csv", index=False)
print("MAEC transcripts:", len(maec_df), "->", RAW/"maec_transcripts.csv")

print("\nALL DOWNLOADS COMPLETE.")