{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 03_text_embeddings.ipynb\n",
    "%pip install -q torch transformers datasets tqdm pandas\n",
    "\n",
    "import torch, pandas as pd, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL  = \"xlm-roberta-base\"\n",
    "BATCH  = 16\n",
    "\n",
    "RAW   = Path(\"data/raw\")\n",
    "PROC  = Path(\"data/processed\")\n",
    "EMB   = Path(\"data/embeddings\"); EMB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "enc = AutoModel.from_pretrained(MODEL).to(DEVICE); enc.eval()\n",
    "\n",
    "def embed_texts(texts):\n",
    "    with torch.no_grad():\n",
    "        tk = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(DEVICE)\n",
    "        out = enc(**tk).last_hidden_state\n",
    "        mask = tk.attention_mask.unsqueeze(-1)\n",
    "        mean = (out*mask).sum(dim=1)/mask.sum(dim=1).clamp(min=1)\n",
    "        return mean.detach().cpu().numpy()\n",
    "\n",
    "def embed_df_texts(df, col, out_path):\n",
    "    arrs = []\n",
    "    texts = df[col].astype(str).tolist()\n",
    "    for i in tqdm(range(0, len(texts), BATCH)):\n",
    "        arrs.append(embed_texts(texts[i:i+BATCH]))\n",
    "    embs = np.vstack(arrs) if arrs else np.zeros((0,768))\n",
    "    np.save(out_path, embs); print(\"Saved:\", out_path, embs.shape)\n",
    "\n",
    "# LendingClub splits\n",
    "lc_train = pd.read_parquet(PROC/\"lc_train.parquet\")\n",
    "lc_valid = pd.read_parquet(PROC/\"lc_valid.parquet\")\n",
    "lc_test  = pd.read_parquet(PROC/\"lc_test.parquet\")\n",
    "for name, df in [(\"train\",lc_train),(\"valid\",lc_valid),(\"test\",lc_test)]:\n",
    "    embed_df_texts(df, \"description\", EMB/f\"lc_{name}_emb.npy\")\n",
    "\n",
    "# Auxiliary corpora (optional enrichment)\n",
    "aux_paths = [\n",
    "    RAW/\"multifin_text.csv\",\n",
    "    RAW/\"multifinben_en_text.csv\",\n",
    "    RAW/\"multifinben_es_text.csv\",\n",
    "    RAW/\"maec_transcripts.csv\",\n",
    "]\n",
    "for p in aux_paths:\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        col = \"text\" if \"text\" in df.columns else df.columns[0]\n",
    "        df = df.sample(min(50000, len(df)), random_state=42)\n",
    "        embed_df_texts(df, col, EMB/f\"{p.stem}_emb.npy\")\n",
    "\n",
    "print(\"Embedding generation complete.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
